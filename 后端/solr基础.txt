                                solr基础
    安装solr：
        1.下载 solr 8.2
            https://archive.apache.org/dist/lucene/solr/8.2.0/solr-8.2.0.zip
            或者
            https://archive.apache.org/dist/lucene/solr/8.2.0/solr-8.2.0.tgz
        2.解压下载的solr压缩包
            unzip solr-8.2.0.zip
            或者
            tar zxf solr-8.2.0.tgz

    启动solr:
        cd  solr-8.2.0
        # 默认使用 standalone 模式，默认监听 8983 端口，使用 -p <portNum> 设置指定端口
        bin/solr start
        #使用root用户启动solr会返回一个warning，提示不安全，如果一定要用root用户启动solr,需要使用 -force 选项

        # 使用 solrcloud 模式启动
        bin/solr start -c
        # 默认情况下，standalone 模式和solrcloud 模式 都使用server/solr/ 作为节点工作目录

        # 使用 SolrCloud 模式启动，可以使用 solr提供的 cloud 交互式示例创建节点
        # bin/solr start -e cloud

        # 手动创建solr实例home目录
        # mkdir -p demo/cloud/node1/solr
        # mkdir -p demo/cloud/node2
        # cp server/solr/solr.xml demo/cloud/node1/solr/
        # cp server/solr/zoo.cfg  demo/cloud/node1/solr/
        # cp demo/cloud/node1/solr demo/cloud/node2/

        # 启动 node1，使用 -s 指定启动的solr实例的home目录
        # bin/solr start -c -p 8983 -s demo/cloud/node1/solr
        # 启动 node2
        # bin/solr start -c -p 7574 -s demo/cloud/node2/solr -z localhost:9983
        # 重启
        # bin/solr restart

    查看状态(也可以通过浏览器访问 http://localhost:8983/solr 查看solr信息)
        bin/solr status
    查看版本
        bin/solr version
    健康检查
        bin/solr healthcheck -c mycollection1 [-z localhost:9983]

    创建 core/collection (standalone模式创建core，SolrCloud模式创建collection)
        # 可以使用 -d 参数指定创建的 core/collection的配置文件目录（包括solrconfig.xml、managed-schema等配置文件），
        # 默认为 _default，位于server/solr/configsets下，SolrCloud模式下指定的配置文件目录将被上传到zookeeper中
        bin/solr create -c mycore
        # SolrCloud模式下创建 collection 可指定 shard 和 replicas
        # bin/solr create -c mycollection1 -s 2 -rf 2 
    删除 core/collection
        # 可以使用 -deleteConfig false 不要删除 zookeeper 上的配置（默认会随 collection 一起删除）
        bin/solr delete -c mycollection1

    修改配置：
        # 设置 mycollection1 的property updateHandler.autoCommit.maxDocs为100
        bin/solr config -c mycollection1 -p 8983 -action set-property -property updateHandler.autoCommit.maxDocs -value 100
        # 取消设置 mycollection1 的 property updateHandler.autoCommit.maxDocs
        bin/solr config -c mycollection1 -p 8983 -action unset-property -property updateHandler.autoCommit.maxDocs
        # 设置用户自定义的property
        bin/solr config -c mycollection1 -p 8983 -action set-user-property -property update.autoCreateFields -value false

    zookeeper相关操作：
        # 查看使用说明
        bin/solr zk -help
        # 上传solr配置 到 zookeeper
        bin/solr zk upconfig -n up_config -d /home/solr/solr_default_conf -z localhost:9983
        # 下载配置
        bin/solr zk downconfig -n rsa -d /home/solr/down_config -z localhost:9983
        # 复制本地文件到zk，可以使用 -r 复制文件夹，另外交换 file:~ 和 zk:~ 的顺序可以从zk复制文件到本地
        bin/solr zk cp file:/usr/local/solr-8.2.0/server/solr/mycore1/conf/managed-schema zk:/configs/up_config -z localhost:9983
        # 删除zk节点
        bin/solr zk rm -r /configs/up_config -z localhost:9983
        # 列出zk节点的子节点
        bin/solr zk ls [-r] /configs/_default -z localhost:9983
        # 创建一个zk节点
        bin/solr zk mkroot /configs/test -z localhost:9983
    
    建立索引：
        # 使用 solr 为文档建立索引(这里使用solr提供的实例数据集)
        bin/post -c mycore example/exampledocs/*
        # bin/post -c mycollection1 example/exampledocs/*
    
    执行搜索：
        # 浏览器访问 http://localhost:8983/solr/mycore/query 页面，在此页面中填写查询内容和其他参数，然后发起搜索
        # 通过curl 发起搜索
        curl http://localhost:8983/solr/mycore/select?q=*:*

    停止solr进程：
        standalone 模式直接使用stop命令和端口参数停止：
            bin/solr stop -p 8983
        也可以使用 -all 选项停止所有solr进程
            bin/solr stop -all

    solr目录结构说明：
        SOLR_HOME
            bin
            server
                solr                            # 默认的solr实例的home目录，保存索引数据，启动时可通过 -s 设置为其他位置
                    solr.xml                    # 当前solr实例的配置，如 支持的组合查询的最大组合条件数，zk连接超时时间等
                    zoo.cfg
                    core1
                        conf                    # standalone模式下 -d 指定的配置文件目录内容被复制这里；solrcloud模式则无此目录，配置文件被上传到zk的/configs/下
                            managed-schema      # 主要是定义需索引文档的 field 和 fieldType以及field对应的analyzer
                            solrconfig.xml      # 定义控制core的一些高层属性，如保存索引的位置等
                            stopwords.txt       # 定义一些analyzer使用的停用词
                            ...
                        core.properties         # core的property，如shard总数，shard编号，name，所属collection等
                        data                    # 保存索引及其他元数据，这是默认位置，可以在solrconfig.xml中修改
                    core2
                solr-webapp                     # 处理请求的代码目录
                lib 
                etc
                logs
                ...
            example                             # 示例
            contrib                             # 第三方库
            dist                                # solr核心库

    Schema文件
    solr使用schema文件来描述需索引文档的field和fieldType，schema文件有两种：managed-schema 和 schema.xml，通过在solrconfig.xml配置
<schemaFactory>来确定使用的schema类型，默认模式下，在solrconfig.xml没有显式配置<schemaFactory>，使用的是managed-schema，配置等效于：
        <schemaFactory class="ManagedIndexSchemaFactory">
            <bool name="mutable">true</bool>
            <str name="managedSchemaResourceName">managed-schema</str>
        </schemaFactory>
    managed-schema支持通过Schema API来动态修改文件内容，不建议手动编辑这个文件。
    schema.xml是传统的schema文件，只能手动修改，并且运行时collection加载后再修改schema.xml后不能直接生效，必须重新加载collection才能生效。
配置使用schema.xml：
        <schemaFactory class="ClassicIndexSchemaFactory"/>
    从managed-schema迁移到schema.xml：
        1.重命名 managed-schema 文件为 schema.xml
        2.在solrconfig.xml删除ManagedIndexSchemaFactory配置，新增<schemaFactory class="ClassicIndexSchemaFactory"/>
        3.重新加载collection
    从schema.xml迁移到managed-schema：
        1.在solrconfig.xml删除ClassicIndexSchemaFactory配置，新增ManagedIndexSchemaFactory配置
        2.重启solr，solr重启后检测到schema.xml，将把这个文件重命名为schema.xml.bak，然后复制该文件并命名为managed-schema
    schemaless模式是在managed-schema的基础上发展出的一种自动根据文档数据分析fieldType并添加field到schema文件的模式。所以要启用此模式，
需要使用managed-schema，然后需要配置field类型猜测处理器，在默认配置中（server/solr/_default）中配置了AddSchemaFieldsUpdateProcessorFactory等
updateProcessor组成的名为add-unknown-fields-to-the-schema的updateRequestProcessorChain，并且默认被应用到所有的更新请求处理器上。可以通过配置
设置property：update.autoCreateFields为false来禁用schemaless模式：
        bin/solr config -c mycollection1 -p 8983 -action set-user-property -property update.autoCreateFields -value false

    schema文件元素配置：
    field配置：
        <field name="fd_test" type="ft_test" indexed="true" stored="true" multiValued="true" />
    fieldtype配置：
        <fieldType name="ft_test" class="solr.TextField" positionIncrementGap="100">
            <analyzer>
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="(\w+)(ing)" replacement="$1"/>
                <tokenizer class="solr.StandardTokenizerFactory"/>
                <filter class="solr.EnglishMinimalStemFilterFactory"/>
            </analyzer>
        </fieldType>
        # 不指定analyzer的type属性则analyzer应用于建立索引和搜索两个过程，如果需要分别为建立索引和搜索使用不同的analyzer，则声明两个analyzer，
          使用type属性值：index和 query指明analyzer应用的过程。
        # charFilter用于在tokenizer创建语汇单元前修改源输入field的值，tokenizer将field的值处理为语汇单元流，filter则对tokenizer生成的语汇单元流进行
          进一步处理。
    copyField配置：
        <copyField source="*" dest="_text_"/>
    这个配置会在分析field之前将所有其他field的内容都复制（追加）到名为 _text_ 的field中，然后为_text_建立索引，这样就可以在查询时使用_text_作为field来避免用户搜索时需要指定
field这样的麻烦，但是这个field会导致索引变得非常大，所以默认配置中没有启用。因为_text_将包含其他field的内容，所以需要设置其property：multiValued为true。
    copyField可以使用通配符：
        <copyField source="*_str" dest="_text_"/>
        <copyField source="*_str" dest="*_text"/>   //这里dest里的*将匹配source里的*
    dynamicField配置：
        <dynamicField name="*_i" type="pint" indexed="true" stored="true"/>
    动态field配置用于匹配需索引文档中未匹配到schema中所有<field>、且name匹配的field，dynamicField指定的fieldType和analysis将应用到文档中匹配的field。
    
    docValues
    docValues是lucene4开始提供的一种保存document-fieldValue的结构，使用docValue可以让facet、sort等操作快速完成和减少内存占用。对于facet来说，如果只使用倒排索引结构，则需要
遍历所有搜索匹配的文档查找统计facetField，使用docValue保存document-facetField则既减少了加载所有匹配文档的内存占用，也减少了遍历文档查找facetField的CPU时间。
    配置docValue:
        <field name="author" type="strings" docValues="true" indexed="false" stored="false" useDocValuesAsStored="false"/>
        # stored通常设置为false，useDocValuesAsStored 不设置则默认为true，为true时，搜索时指定返回所有field(fl=*)则会返回此field，如果设为false，要返回此field则需要指明（fl=*,author）
    也可以为动态field规则配置使用docValue
    docValues只能应用于部分fieldType上：StrField、UUIDField、*PointField、BoolField、Date fields、EnumFieldType、CurrencyFieldType

    Schema API
    Schema API支持通过http请求修改schema文件（managed-schema文件），在每次修改后，core或collection会自动重新加载（schema文件被重新加载）。
    现代solr的API支持两个版本：V1和V2
    操作schema的API：
        Schema API V1:
            POST    http://localhost:8983/solr/coreName/schema
            http://localhost:8983/solr/collectionName/schema
        Schema API V2:
            POST    http://localhost:8983/api/cores/coreName/schema
            POST    http://localhost:8983/api/collections/collectionName/schema
    Schema API通过参数中的命令来指定具体需要执行的动作，这些命令包括：
        add-field               新增一个field
        delete-field            删除一个field
        replace-field           替换一个field，替换时需要给出该field的所有property，因为采用的是完全替换而不是部分替换，被替换的field必须存在，否则将报错
        add-dynamic-field       添加要给动态field规则    
        delete-dynamic-field    删除一个动态field规则
        replace-dynamic-field   替换一个动态field规则
        add-field-type          新增一个fieldType
        delete-field-type       删除一个fieldType
        replace-field-type      替换一个fieldType
        add-copy-field          新增一个copyField规则
        lete-copy-field         删除一个copyField规则
    通过Schema API V1向schema文件新增field实例：
        curl -X POST -H 'Content-type:application/json' --data '{
        "add-field":{
        "name":"sell_by",
        "type":"pdate",
        "stored":true }
        }' http://localhost:8983/solr/mycollection1/schema
    通过Schema API V2向schema文件新增field实例：
        curl -X POST -H 'Content-type:application/json' --data '{
        "add-field":{
        "name":"sell_by",
        "type":"pdate",
        "stored":true }
        }' http://localhost:8983/api/collections/mycollection1/schema
    删除field：
        curl -X POST -H 'Content-type:application/json' --data '{
        "delete-field" : { "name":"sell_by" }
        }' http://localhost:8983/api/collections/mycollection1/schema

    获取完整schema内容的API：
        V1:
            GET     http://localhost:8983/solr/coreName/schema
            GET     http://localhost:8983/solr/collectionName/schema
        V2：
            GET     http://localhost:8983/api/cores/coreName/schema
            GET     http://localhost:8983/api/collections/collectionName/schema
    默认返回格式为json，可以通过wt参数指定返回的格式：
        ?wt=xml             使用xml格式
        ?wt=schema.xml      直接以schema文件格式返回
    获取schema的所有field：
        curl http://localhost:8983/solr/mycollection1/schema/fields
    获取指定field：
        curl http://localhost:8983/solr/mycollection1/schema/fields/name
    获取schema的所有动态field规则
        curl http://localhost:8983/solr/mycollection1/schema/dynamicfields
    获取指定动态field规则
        curl http://localhost:8983/solr/mycollection1/schema/dynamicfields/name
    获取schema的所有copyField规则
        curl http://localhost:8983/solr/mycollection1/schema/copyfields
    获取指定copyField规则
        curl http://localhost:8983/solr/mycollection1/schema/copyfields/name
    获取schema的所有fieldType
        curl http://localhost:8983/solr/mycollection1/schema/fieldtypes
    获取指定fieldType
        curl http://localhost:8983/solr/mycollection1/schema/fieldtypes/name
    
    其他Schema API:
        GET /collection/schema/name
        GET /collection/schema/version
        GET /collection/schema/uniquekey
        GET /collection/schema/similarity

    修改了schema文件后，通常需要重新索引以前的文档，重建索引的几种策略：
        1.删除原lucene索引后重新索引文档
            # 通过http请求删除collection的索引，不会马上删除lucene索引，需要重新加载collection才会删除索引
            curl -X POST -H 'Content-Type: application/json' --data '{"delete":{"query":"*:*" }}' http://localhost:8983/solr/mycollection1/update
        2.索引文档到新的collection后为该新collection1创建别名

    重新加载core/collection（的schema文件）：
        curl "http://localhost:8983/solr/admin/cores?action=RELOAD&core=mycore1"
        curl "http://localhost:8983/solr/admin/collections?action=RELOAD&name=mycollection1"

    Collections API
    Collections API支持通过http请求操作collection：
    为collection创建别名：
        curl "http://localhost:8983/solr/admin/collections?action=CREATEALIAS&collections=mycollection1&name=mc2&wt=json"
    别名可以用于查询，相当于一个逻辑collection，并且一个别名可以映射到多个标准collection，这样通过使用别名就把业务逻辑和具体的collection索引分离开了，因而可以在运行时通过修改别名
指向新的标准collection，实现索引的替换，这可以应用在重建索引和按时间段建立索引的情境中（solr支持通过collection API参数实现动态配置时间段索引）。
    删除别名：
        curl "http://localhost:8983/solr/admin/collections?action=DELETEALIAS&name=mc1&wt=json"
    删除collection：
        curl "http://localhost:8983/solr/admin/collections?action=DELETE&name=mycollection1"
    删除core:
        curl "http://localhost:8983/solr/admin/cores?action=UNLOAD&core=mycore2&deleteIndex=true&deleteDataDir=true&deleteInstanceDir=true"
    重新加载collection：
        curl "http://localhost:8983/solr/admin/collections?action=RELOAD&name=mycollection1"

    Config API
        http://localhost:8983/solr/mycollection1/config
    
    faceting
    facet是solr对搜索返回数据的分组统计实现，可以在搜索时开启facet：/select?facet=on&facet.field=fd1&facet.field=fd2

    搜索查询
    SearchRequestHandlers使用查询解析器（query parser）处理搜索关键词，主要的 query parser包括：
        The Standard Query Parser
        The DisMax Query Parser
        The Extended DisMax Query Parser
    所有query parser都支持的公共查询参数：
        defType：指定查询解析器（query parser）：defType=lucene/dismax
        sort：指定排序：sort=name asc | sort=name asc,price desc | sort=field(name,min) asc/desc #用于multiValued="true"
        start：指定返回查找到的文档集的子集的offset，默认为0，这个参数可以用于分页：start=10
        rows：指定一次查询返回的文档数的最大值，默认为10：rows=20
        fq：filter query，solr同时执行q和fq，然后缓存fq返回的文档，后续使用相同fq的搜索会直接在此fq缓存的文档集合里搜索，
            可以加快搜索速度：fq=price:[10.0 TO 100.0]&popularity:1 也可以使用Bool查询语法：fq=+price:[10.0 TO 100.0] +popularity:1
        fl：指定返回的field：fl=name cat | fl=name,cat | fl=* score 返回所有stored=true的field和文档的score
        wt：指定Response Writer：wt=json/xml
        cache：指定是否缓存搜索结果，solr默认缓存所有query和filter query的结果；停用缓存：cache=false
    
    solr默认使用 The Standard Query Parser，也称为lucene parser.
    常用参数：
        q：查询参数
        q.op 指定查询表达式的默认行为，将覆盖solrconfig.xml中的定义，只能是AND或OR
        df：默认查询field，将覆盖solrconfig.xml中的定义
    通配符查询：
        // ?匹配单个字符，*匹配多个字符
        q=f1:hello | q=f1:hell? |  q=f1:he*
    模糊查询：
        // ~n 表示可以替换（包括增加删除）的字符数，默认为2
        q=f1:hella~ | q=f1:hillo~1
    临近搜索：
        // ~n 表示将索引中与查询短语中匹配的term移动到查询短语中相同的位置允许的最大移动次数
        // "name":"Test with some GB18030 encoded characters",
        q=name:"some characters"~2
        q=name:"test some characters"~3
    范围搜索：
        // 字符串使用字符序，大小写都会被转为小写处理，使用通配符则无论使用}还是]，作用都是}
        q=f1:[10 TO 20]  | q=f1:[10 TO 20}    | q=f2:[apple TO berlin]  | q=f2:[a* TO b*]
    term加权搜索：
        // ^n 为前面的term增加一个权重因子n
        q=name:ipod^5 apple | f1:"t1 t2"^4 "t3 t4"
    为特定查询条件设置固定score：
        q=(f1:hello)^=5 OR f2:solr
    组合查询：
        // AND 交集，也可以使用 &&
        q=f1:hello AND f2:solr
        // OR 并集，也可以使用 ||，是lucene parser的默认操作符
        q=f1:hello || f2:solr
        // NOT 差集，也可以使用 !
        q=f1:hello ! f2:solr
        // + 要求后面的条件必须满足
        q=f1:hello +f2:solr
        // - 要求后面的条件一定不匹配
        q=f1:hello -f2:solr
    DisMax parser只使用 + -两种操作符
    查询中的特殊字符：
        + - && || ! ( ) { } [ ] ^ " ~ * ? : /
    特殊字符转义使用 \
    在一个field中使用组合条件：
        q=f1:(+hello +"solr core")
    查询条件中可以使用/*...*/注释，/和*都需要转义
    组合查询中使用filter()缓存：
        q=f1:hello +filter(f2:solr)
    日期时间：
        q=f3:2006-02-13T15\:26\:37Z
        q=f3:"2006-02-13T15:26:37Z"
        q=f3:[2006-02-13T15:26:37Z TO NOW]
    
    dismax parser常用参数：
        q       查询参数
        q.alt   q为空时，把q.alt作为q传递给lucene parser
        qf      查询q中关键词的field
        mm      最小匹配条件数
        pf      查询短语加权field
        qs      查询短语slot，在field中为了匹配查询短语允许移动的最大次数，ps与此相同
        tie     用于文档得分计算，默认为0，文档得分=查询匹配最高分 + tie*(其他查询条件得分之和)
        bq      加权查询条件，用于将满足此查询条件的文档的得分加到q查询的当前文档的得分
        bf      加权函数，计算匹配程度得到一个加权分数加到原分数上
                recip(rord(myfield),1,1001,1000)
                # 1001/(q中term在myfield中的逆位序*1+1000)
    
    Local Parameters in Queries
    q查询条件中可以使用 {!...}来指定其他查询条件，条件通过key=value指定，如果没有指定key，则默认key为type，表示查询解析器：
        q={!df=name}drive
        q={!dismax qf=name}drive
        q={!type=dismax qf=name v='drive'}
        q={!df=name v=$qq}&qq=drive
    
    函数查询
    函数查询可以用在q中调整文档得分，也可以用在fl中构造一个pseudo-field，也可以用于排序：
        // 把popularity的值作为得分加到返回文档的得分上，_val_是在查询中使用函数的语法
        q=price:{0 TO *] AND _val_:"def(popularity,0)"
        // 返回pseudo-field
        fl=*,score,div(price,def(popularity,1))
        // 排序
        sort=product(price,def(popularity,1)) desc
    常用函数：
        def(f1,n)   返回f1的值，不存在则返回默认值n
        div(a,b)    a/b
        product(a,b,c,...)  a*b*c*...
        ord(f1)     匹配文档的f1的索引序
        rord(f1)    ord的逆序
        recip(x,m,a,b)     a/(m*x+b) 
        sum(a,b,c,...)
        
